# ICCV-prompt 2023
## 文章总览
1. PROMPTCAP: Prompt-Guided Image Captioning for VQA with GPT-3 https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_PromptCap_Prompt-Guided_Image_Captioning_for_VQA_with_GPT-3_ICCV_2023_paper.pdf
   基于知识的视觉问答(VQA)涉及需要图像之外的世界知识来产生正确答案的问题。像GPT-3这样的大型语言模型(LMs)由于其强大的知识检索和推理能力，对这项任务特别有帮助。为了使LM能够理解图像，之前的工作使用图像描述模型将图像转换为文本。然而，当在一个单独的标题句子中总结图像时，要描述的视觉实体往往没有明确。通用的图像描述通常会错过视觉细节，这对LM正确回答视觉问题至关重要。为了应对这一挑战，我们提出了PROMPTCAP(提示引导图像描述)，这是一个描述模型，旨在作为图像和黑盒lm之间更好的连接器。与一般的标题不同，**PROMPTCAP使用自然语言提示符来控制生成的标题中要描述的可视化实体**。提示包含一个问题，标题应该有助于回答这个问题。为了避免额外的注释，PROMPTCAP通过GPT-3和现有数据集合成的示例进行训练。我们演示了PROMPTCAP在现有管道上的有效性，其中GPT-3被提示使用图像标题来执行VQA。PROMPTCAP在基于知识的VQA任务上的准确率大大优于一般的图像描述方法。**基于知识的VQA，图像描述结合LLM**
2. Generating Instance-level Prompts for Rehearsal-free Continual Learning https://openaccess.thecvf.com/content/ICCV2023/papers/Jung_Generating_Instance-level_Prompts_for_Rehearsal-free_Continual_Learning_ICCV_2023_paper.pdf
   本文介绍了一种利用视觉transformer(ViT)进行持续学习的新方法——域自适应提示(DAP)。基于提示的持续学习最近因其无需rehearsal的性质而受到关注。目前，基于提示的持续学习提出的提示池是在一系列任务中有效利用冻结的预训练ViT骨干的关键。然而，我们观察到提示池的使用在预训练和持续学习之间产生了域可扩展性问题。这个问题是由于提示池中组级指令的固有编码导致的。为了解决这个问题，我们提出了DAP，这是一种无池的方法，可以在推理时以实例级的方式生成适当的提示。我们优化了一个自适应提示生成器，它为每个输入创建特定于实例的细粒度指令，从而增强了模型的可塑性，减少了遗忘。我们在七个与ImageNet具有不同程度域相似度的数据集上进行的实验表明，DAP优于最先进的基于提示的方法。**持续学习，生成式提示，冻结的预训练ViT**
3. Space-time Prompting for Video Class-incremental Learning https://openaccess.thecvf.com/content/ICCV2023/papers/Pei_Space-time_Prompting_for_Video_Class-incremental_Learning_ICCV_2023_paper.pdf
   近年来，基于提示的学习在图像类增量学习方面取得了令人瞩目的进展，但在视频领域还缺乏足够的探索。在本文中，我们将通过基于强大的图像语言预训练模型(即CLIP)学习多个提示来填补这一空白，使其适合视频类增量学习(VCIL)。为此，我们提出了一种时空提示方法(ST-Prompt)，它包含两种提示，即任务特定提示和任务不可知提示。任务特定提示是通过学习多粒度提示，即空间提示、时间提示和综合提示，来解决灾难性遗忘问题，实现对任务的准确识别。任务不可知提示维持一个全局共享提示池，这可以通过在帧之间交换上下文来赋予预训练的图像模型时间感知能力。通过这种方式，ST-Prompt可以将图像语言预训练模型中的丰富知识转移到VCIL任务中，只需要优化一小部分提示符。为了评估ST-Prompt，我们在三个标准基准上进行了广泛的实验。结果表明，ST-Prompt在1 × 25阶段设置下，在HMDB51数据集上的准确率提高了9.06%，显著优于当前的VCIL方法.**时间提示（任务特定提示）+空间提示（任务不可知提示），视频增量学习**
4. Spatio-temporal Prompting Network for Robust Video Feature Extraction https://openaccess.thecvf.com/content/ICCV2023/papers/Sun_Spatio-temporal_Prompting_Network_for_Robust_Video_Feature_Extraction_ICCV_2023_paper.pdf
   帧质量下降是视频理解领域面临的主要挑战之一。为了弥补由于帧退化造成的信息损失，最近的方法利用基于变压器的集成模块来获取时空信息。然而，这些集成模块沉重而复杂。此外，每个集成模块都是专门为其目标任务量身定制的，因此很难推广到多个任务。在本文中，我们提出了一个整洁统一的框架，称为时空提示网络(STPN)。通过**对骨干网输入特征的动态调整**，可以有效提取鲁棒准确的视频特征。具体来说，STPN预测多个包含相邻帧时空信息的视频提示。然后，将这些视频提示添加到当前帧的补丁嵌入中，作为视频特征提取的更新输入。此外，STPN很容易推广到各种视频任务，因为它不包含特定于任务的模块。STPN在三个广泛使用的数据集上实现了最先进的性能，用于不同的视频理解任务，即用于视频对象检测的ImageNetVID，用于视频实例分割的YouTubeVIS和用于视觉对象跟踪的GOT-10k。**时空提示，多任务，泛化能力强**
5. Exploring the Benefits of Visual Prompting in Differential Privacy https://arxiv.org/pdf/2303.12247.pdf
   视觉提示(VP)是一种新兴而强大的技术，通过设计训练有素的冷冻源模型，使样本高效适应下游任务。在这项工作中，我们探讨了VP在构建具有差分隐私(DP)的令人信服的神经网络分类器中的好处。我们探索并将VP整合到规范的DP训练方法中，并证明了它的简单和高效。特别是，我们发现VP与PATE(一种最先进的DP培训方法，利用教师群体的知识转移)相结合，以最小的隐私预算支出实现了最先进的隐私-效用权衡。此外，我们还对具有足够的域间隙的跨域图像分类进行了额外的实验，进一步揭示了VP在DP中的优势。最后，我们还进行了广泛的消融研究，以验证VP在DP考虑下的有效性和贡献。**差分隐私?**
6. Decouple Before Interact: Multi-Modal Prompt Learning for Continual Visual Question Answering https://openaccess.thecvf.com/content/ICCV2023/papers/Qian_Decouple_Before_Interact_Multi-Modal_Prompt_Learning_for_Continual_Visual_Question_ICCV_2023_paper.pdf
   在现实世界中，期望一个理想的视觉问答模型能够在连续设置中为新问题和图像提供正确的答案(称为CL-VQA)。然而，现有的研究从视觉或语言的角度来制定CLVQA，并直接将单模态持续学习(CL)策略应用于这种多模态任务，这是不合适的和次优的。一方面，这种片面的表述可能导致有限的评价。另一方面，忽视模态之间的相互作用将导致较差的性能。为了解决这些具有挑战性的问题，我们从多模态视觉语言融合的角度提出了一种CL-VQA的综合表述。在此基础上，我们进一步提出了基于交互前解耦的多模态提示学习(TRIPLET)，这是一种基于预训练的视觉语言模型的新方法，由解耦提示和提示交互策略组成，以捕获模态之间的复杂交互。特别是，解耦的提示包含可学习的参数，这些参数在不同方面被解耦，提示交互策略负责对输入和提示之间的交互建模。此外，我们构建了两个CL-VQA基准，以进行更全面的评估。大量的实验表明，我们的TRIPLET在CL-VQA的单模态和多模态连续设置中都优于最先进的方法。**多模态学习，交互前解耦，视觉语言融合**
7. Efficient Model Personalization in Federated Learning via Client-Specific Prompt Generation  https://arxiv.org/pdf/2308.15367.pdf
   联邦学习(FL)作为一种分散的学习框架出现，它在不共享数据以保护隐私的情况下从多个分布式客户端训练模型。最近，大规模的预训练模型(例如，Vision Transformer)已经显示出强大的鲁棒表示能力。然而，客户端之间的数据异构性、有限的计算资源和通信带宽限制了大规模模型在FL框架中的部署。为了利用大规模模型的鲁棒表示，同时为异构客户端实现有效的模型个性化，我们提出了一种新的客户端特定提示生成(pFedPG)的个性化FL框架，该框架学习在服务器上部署个性化提示生成器，以生成客户端特定的视觉提示，从而有效地将冻结的骨架适应本地数据分布。我们提出的框架共同优化了个性化提示适应的局部阶段和个性化提示生成的全局阶段。前者旨在训练视觉提示，使基础模型适应每个客户端，后者观察局部优化方向，为所有客户端生成个性化提示。通过在基准数据集上的大量实验，我们表明，在各种类型的数据异构下，我们的pFedPG优于最先进的个性化FL方法，允许计算和通信高效的模型个性化。**联邦学习，特定生成提示，局部阶段生成和全局阶段生成**
8. LoGoPrompt: Synthetic Text Images Can Be Good Visual Prompts for Vision-Language Models https://arxiv.org/pdf/2309.01155.pdf
   提示工程是一种强大的工具，用于提高预训练模型在下游任务中的性能。例如，提供提示“让我们一步一步地思考”将GPT-3在multiarith上的推理准确率提高到63%，而提示“一张照片”填充类名使CLIP在ImageNet上达到80%的零射击准确率。虽然以前的研究已经探索了视觉模态的提示学习，但分析什么是专门用于图像识别的良好视觉提示是有限的。此外，现有的视觉提示调优方法的泛化能力不如纯文本提示调优。本文探讨了我们的关键见解:合成文本图像是视觉语言模型的良好视觉提示!为了实现这一目标，我们提出了LoGoPrompt，它将**分类目标重新定义为视觉提示选择**，并解决了先添加合成文本图像作为分类明智的视觉提示或先预测类别的鸡和蛋的挑战。在没有任何可训练的视觉提示参数的情况下，在16个数据集上的实验结果表明，我们的方法在小样本学习、从基础到新泛化和领域泛化方面始终优于最先进的方法。**合成文本图像，视觉语言模型，重新定义分类任务**
9. Instance-aware Dynamic Prompt Tuning for Pre-trained Point Cloud Models https://arxiv.org/pdf/2304.07221.pdf
    预训练的点云模型在物体分类和零件分割等3D理解任务中得到了广泛的应用。然而，在下游任务中普遍采用的完全微调策略导致每个任务的模型参数存储开销很大，这限制了应用大规模预训练模型时的效率。受最近成功的视觉提示调优(VPT)的启发，本文试图探索预训练点云模型的提示调优，以追求性能和参数效率之间的优雅平衡。我们发现，尽管实例无关的静态提示(例如VPT)在下游传输中显示出一定的效果，但它容易受到现实世界点云数据中各种类型噪声引起的分布多样性的影响。为了克服这一限制，我们提出了一种新的针对预训练点云模型的实例感知动态提示调优(IDPT)策略。IDPT的本质是开发动态提示生成模块，感知每个点云实例的语义先验特征，并生成自适应提示令牌，增强模型的鲁棒性。值得注意的是，大量的实验表明，IDPT在大多数任务中仅使用7%的可训练参数就优于完全微调，为预训练点云模型的参数高效学习提供了一个有希望的解决方案。**3D点云，动态提示生成**
10. PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization https://arxiv.org/pdf/2307.15199.pdf
    在联合视觉语言空间中，文本特征(例如来自“a photo of dog”)可以有效地表示其相关的图像特征(例如来自狗的照片)。此外，最近的一项研究已经证明了这种联合空间的跨模态可转移现象。根据这些观察，我们提出了PromptStyler，它通过提示合成不同的样式来模拟联合空间中的各种分布变化，而不使用任何图像来处理无源域泛化。该方法通过伪词S*的可学习风格词向量来学习生成各种风格特征(从“a S* style of a”)。为了确保学习到的样式不会扭曲内容信息，我们强制样式-内容特征(来自“a S* style of a [class]”)位于联合视觉语言空间中与其对应的内容特征(from "[class]")附近。在学习风格词向量后，我们使用综合风格-内容特征训练线性分类器。PromptStyler在PACS, VLCS, OfficeHome和DomainNet上实现了最先进的状态，尽管它不需要任何图像进行培训。**视觉语言，纯文本prompt**
11. Self-regulating Prompts: Foundational Model Adaptation without Forgetting https://arxiv.org/pdf/2307.06948.pdf
    对于各种下游任务，快速学习已经成为对基础模型(如CLIP)进行微调的有效替代方法。传统上使用特定于任务的目标进行训练，即交叉熵损失，提示倾向于过度拟合下游数据分布，并且发现从冻结的CLIP中捕获与任务无关的一般特征具有挑战性。这导致了模型原有泛化能力的丧失。为了解决这个问题，我们的工作引入了一个名为PromptSRC(带有自我调节约束的提示)的提示自正则化框架。PromptSRC使用三管齐下的方法来指导提示优化任务特定和任务不可知的一般表征:(a)通过与冻结模型的相互协议最大化来调节提示表征，(b)通过训练轨迹上的提示自集合进行调节，以编码其互补优势，(c)通过文本多样性进行调节，以缓解与视觉分支的样本多样性失衡。据我们所知，这是第一个用于提示学习的正则化框架，它通过共同关注预训练的模型特征、提示期间的训练轨迹和文本多样性来避免过拟合。PromptSRC显式地引导提示学习一个表示空间，在不影响CLIP泛化的情况下最大化下游任务的性能。我们在4个基准测试中进行了广泛的实验，与现有方法相比，PromptSRC总体上表现良好。。**提示自正则化框架，自我调节约束**
12. Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning https://arxiv.org/pdf/2308.06038.pdf
    得益于提示调优，近年来，预训练的视觉语言模型(例如CLIP)在多用途下游任务上表现良好。在本文中，我们关注于一种特殊的学习自适应提示设置，这种设置是针对来自未知新域的每个测试样本的，这被称为测试时间提示调优(TPT)。现有的TPT方法通常依赖于数据增强和置信度选择。然而，传统的数据增强技术，如随机调整作物大小，缺乏数据多样性，而基于熵的置信度选择本身不足以保证预测的保真度。为了解决这些问题，我们提出了一种新的TPT方法，称为DiffTPT，它利用预训练的扩散模型来生成多样化和信息丰富的新数据。具体来说，我们结合了传统方法和预训练稳定扩散的增强数据，利用它们各自的优点，提高了模型对未知新测试数据的适应能力。此外，为了保证生成数据的预测保真度，我们引入了基于余弦相似度的过滤技术，以选择与单个测试样本相似度较高的生成数据。我们在具有分布移位和未见类别的测试数据集上的实验表明，与最先进的TPT方法相比，DiffTPT将零射击精度平均提高了5.13%。我们的代码和模型将会公开发布。**test-time prompt tuning (TPT),数据增强**
13. Gradient-Regulated Meta-Prompt Learning for Generalizable Vision-Language Models https://arxiv.org/pdf/2303.06571.pdf
    提示调整是最近出现的一种范例，通过学习“软提示”来调节冻结的预训练模型，使强大的视觉语言预训练模型能够以参数和数据有效的方式适应下游任务。虽然有效，但在few-shot场景下问题特别大，在这种场景下，**提示调优性能对初始化很敏感**，需要一个耗时的过程来找到一个好的初始化，从而限制了预训练模型的快速适应能力。此外，提示调优可能会破坏预训练模型的泛化性，因为可学习的提示令牌容易过拟合到有限的训练样本。为了解决这些问题，我们引入了一种新的梯度调节元提示学习(GRAM)框架，该框架仅使用未标记的图像-文本预训练数据，在元学习范式中联合元学习有效的软提示初始化以更好地适应和轻量级梯度调节功能以实现强跨域泛化。我们的GRAM不需要设计一个特定的提示调优方法，而是可以很容易地以一种与模型无关的方式整合到各种提示调优方法中，综合实验表明，在11个数据集的几种设置(即少镜头学习、跨域泛化、跨数据集泛化等)中，GRAM带来了一致的改进。此外，实验表明，GRAM使文本和视觉提示调优的正交方法以一种相互增强的方式工作，提供了比单模态提示调优方法更好的泛化性。**图像文本，元学习，梯度调节**
14. What Does a Platypus Look Like? Generating Customized Prompts for Zero-Shot Image Classification https://arxiv.org/pdf/2209.03320.pdf
    开放词汇表模型是一种很有前途的图像分类新范式。与传统的分类模型不同，开放词汇模型在推理过程中使用自然语言指定的任意类别集进行分类。这种自然语言称为“提示”，通常由一组手写模板(例如，“a photo of a {}”)组成，其中包含每个类别名称。这项工作引入了一种简单的方法来生成更高精度的提示，而不依赖于任何任务领域的明确知识，并且手工构建的句子要少得多。为了实现这一点，我们将开放词汇模型与大型语言模型(llm)结合起来，通过语言模型(CuPL，发音为“couple”)创建自定义提示。特别是，我们利用llm中包含的知识来生成许多描述性句子，这些句子包含图像类别的重要区分特征。这使得模型在进行预测时更重视图像中的这些区域。我们发现，这种简单而通用的方法提高了一系列零射击图像分类基准的准确性，包括在ImageNet上获得超过一个百分点的增益。最后，这个简单的基线不需要额外的训练，并且完全保持零射击。**开放词表模型，大语言模型，零样本分类**
15. A Retrospect to Multi-prompt Learning across Vision and Language https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_A_Retrospect_to_Multi-prompt_Learning_across_Vision_and_Language_ICCV_2023_paper.pdf
    随着视觉语言预训练模型(VLMs)的出现，视觉领域正经历着前所未有的发展。提示学习是实现vlm的关键，因为它使vlm能够在有限的资源下快速适应下游任务。然而，现有的研究围绕单提示范式进行，很少调查其多提示学习背后的技术潜力。本文旨在为视觉语言多提示学习提供一个有原则的回顾。我们将最近的常模态差距现象扩展到可学习提示，然后从实证和理论上证明了多提示增强视觉语言迁移的优越性。根据这一观察结果，我们提出了一种基于能量的多提示学习(EMPL)，通过从vlm隐式定义的基于能量的分布中提取实例来生成多个提示嵌入。因此，我们的EMPL不仅是参数有效的，而且严格地实现了域内和域外开放词汇泛化之间的平衡。我们进行了全面的实验来证明我们的主张和EMPL的优越性。**Review，视觉语言模型，多提示学习**
16. Prompt Switch: Efficient CLIP Adaptation for Text-Video Retrieval https://arxiv.org/pdf/2308.07648.pdf
    在文本-视频检索中，最近的工作得益于预先训练的文本-图像基础模型(例如CLIP)的强大学习能力，将其适应于视频领域。如何利用CLIP图像编码器有效地捕获视频中丰富的语义是一个关键问题。为了解决这个问题，最先进的方法采用复杂的跨模态建模技术将文本信息融合到视频帧表示中，然而，这在大规模检索系统中引起了严重的效率问题，因为每个文本查询都必须在线重新计算视频表示。在本文中，我们抛弃了这种有问题的跨模态融合过程，并旨在纯粹从视频中学习语义增强的表示，以便视频表示可以离线计算并用于不同文本。具体来说，我们首先在CLIP图像编码器中引入一个时空“提示立方体”，并在编码器层中迭代切换它，以有效地将全局视频语义整合到帧表示中。然后，我们提出应用辅助视频字幕目标来训练帧表示，通过在语义空间中提供细粒度指导，从而促进详细视频语义的学习。在增强的帧表示上使用朴素的时间融合策略(即均值池化)，我们在三个基准数据集(即MSR-VTT, MSVD和LSMDC)上获得了最先进的性能。**文本视频检索，纯视频提示**
17. What does CLIP know about a red circle? Visual prompt engineering for VLMs https://arxiv.org/pdf/2304.06712.pdf
    大规模视觉语言模型，如CLIP，学习强大的图像-文本表示，这些表示已经找到了许多应用，从零样本分类到文本到图像生成。尽管如此，他们通过提示解决新的判别任务的能力仍落后于大型语言模型，如GPT-3。在这里，我们探讨了视觉提示工程的思想，通过在图像空间而不是文本中编辑来解决分类之外的计算机视觉任务。特别是，我们发现CLIP的一种**涌现能力**，通过简单地在物体周围画一个红圈，我们可以将模型的注意力引导到该区域，同时也保持全局信息。我们通过实现最先进的零射击引用表达式理解和关键点定位任务的强大性能，展示了这种简单方法的强大功能。最后，我们提请注意大型语言视觉模型的一些潜在的伦理问题。**Clip + 关键点定位，零样本任务**
18. Generative Prompt Model for Weakly Supervised Object Localization https://arxiv.org/pdf/2307.09756.pdf
    当从图像类别标签中学习对象定位模型时，弱监督对象定位(WSOL)仍然具有挑战性。传统的鉴别训练激活模型的方法忽略了具有代表性但鉴别性较差的对象部分。在这项研究中，我们提出了一个生成提示模型(GenPromp)，定义了第一个pipeline，通过将WSOL制定为条件图像去噪过程来定位不太区分的对象部分。在训练过程中，GenPromp将图像类别标签转换为可学习的提示嵌入，并将这些提示嵌入馈送到生成模型中，以有条件地恢复带有噪声的输入图像并学习代表性嵌入。在推理过程中，enPromp结合了代表性嵌入和判别嵌入(从现成的视觉语言模型查询)，以获得代表性和判别能力。最后利用组合嵌入生成多尺度高质量的注意图，便于对目标的全范围进行定位。在CUB-200-2011和ILSVRC上的实验表明，GenPromp分别比最佳判别模型高出5.2%和5.6% (Top-1 Loc)，为生成模型的WSOL设置了坚实的基线。**弱监督，生成式提示**
19. Iterative Prompt Learning for Unsupervised Backlit Image Enhancement https://arxiv.org/pdf/2303.17569.pdf
    我们通过探索对比语言-图像预训练(CLIP)在像素级图像增强方面的潜力，提出了一种新的无监督背光图像增强方法，简称CLIP- lit。我们发现，开放世界CLIP先验不仅有助于区分背光和光照良好的图像，而且还有助于感知不同亮度的异构区域，促进增强网络的优化。与高级任务和图像处理任务不同，直接将CLIP应用于增强任务是非常重要的，因为很难找到准确的提示。为了解决这个问题，我们设计了一个提示学习框架，该框架首先通过约束提示(阴性/阳性样本)与CLIP潜在空间中相应图像(背光图像/明亮图像)之间的文本-图像相似性来学习初始提示对。然后，我们基于增强结果与初始提示对之间的文本-图像相似性来训练增强网络。为了进一步提高初始提示对的准确性，我们迭代微调提示学习框架，通过秩学习减少背光图像、增强结果和光照良好图像之间的分布差距，提高增强性能。我们的方法在更新提示学习框架和增强网络之间交替进行，直到获得视觉上令人满意的结果。大量的实验表明，我们的方法在视觉质量和泛化能力方面优于最先进的方法，而不需要任何配对数据。**背光图像增强，无需配对数据**
20. PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning https://arxiv.org/pdf/2211.11682.pdf
    大规模的预训练模型在视觉和语言任务中都显示出有希望的开放世界性能。然而，它们在三维点云上的传输能力仍然有限，仅局限于分类任务。在本文中，我们首先将CLIP和GPT合作成为一个统一的3D开放世界学习器，命名为PointCLIP V2，充分释放了它们在零样本3D分类，分割和检测方面的潜力。为了更好地将3D数据与预先训练的语言知识对齐，PointCLIP V2包含两个关键设计。在视觉方面，我们通过形状投影模块提示CLIP生成更逼真的深度图，缩小投影点云与自然图像之间的域差距。对于文本端，我们提示GPT模型生成3d特定文本作为CLIP文本编码器的输入。在没有任何3D领域训练的情况下，我们的方法在三个数据集上的零样本3D分类准确率显著超过PointCLIP +42.90%， +40.44%和+28.75%。此外，V2还可以简单地扩展到少镜头3D分类、零镜头3D部分分割、3D目标检测等领域，展示了我们对统一的3D开放世界学习的泛化能力。**Clip+GPT，三维点云，零样本学习**
21. Order-Prompted Tag Sequence Generation for Video Tagging https://openaccess.thecvf.com/content/ICCV2023/papers/Ma_Order-Prompted_Tag_Sequence_Generation_for_Video_Tagging_ICCV_2023_paper.pdf
    视频标记旨在为给定的视频推断出跨越相关内容的多个标记。视频标签通常由各种用户自由定义和上传，因此具有数量丰富和视频内无序的两个特点。现有的多标签分类和生成方法很难直接适应这一任务。基于上述特点，本文提出了一种新的生成模型——order - prompt Tag Sequence Generation (OP-TSG)。它**将视频标记看作是一个由样本依赖顺序提示引导的标记序列生成问题**。这些提示在语义上与标记对齐，并支持解耦标记生成顺序，使模型专注于对标记依赖项进行建模。此外，基于词的生成策略使模型能够生成新的标签。为了验证该方法的有效性和泛化性，分别建立了中文视频标记基准CREATE-tagging和英文图像标记基准Pexel-tagging。大量的实验结果表明，OP-TSG明显优于其他方法，特别是在罕见标签上的结果比SOTA方法在CREATE-tagging和pixel -tagging上分别提高了3.3%和3%，在CREATE-tagging上生成的新标签的标签增益达到了7.04%.**分类问题建模为生成问题**
22. Towards Unifying Medical Vision-and-Language Pre-Training via Soft Prompts https://arxiv.org/pdf/2302.08958.pdf 
    医学视觉和语言预训练(Med-VLP)由于其适用于从医学图像和文本中提取通用表征，在许多下游医疗任务中显示出有希望的改进。实际上，根据是否使用重型融合模块，有两种典型的类型，\textit{即}融合编码器型和双编码器型。前者在多模态任务中具有优势，因为模态之间有充分的相互作用;后者由于具有单模态编码能力，擅长单模态和跨模态任务。为了利用这两种类型，我们提出了一个有效而简单的方案PTUnifier来统一这两种类型。我们首先通过引入视觉和文本提示来统一输入格式，它们作为存储最具代表性的图像/文本的特征库。通过这样做，单个模型可以作为处理采用不同输入格式(\textit{即}，纯图像、纯文本和图像-文本对)的各种任务的\textit{基础模型}。此外，我们构建了一个提示池(而不是静态提示池)，以提高多样性和可扩展性。实验结果表明，我们的方法在广泛的任务上取得了最先进的结果，包括单模态任务(\textit{即}图像/文本分类和文本摘要)、跨模态任务(\textit{即}图像到文本生成和图像-文本/文本-图像检索)和多模态任务(\textit{即}视觉问答)，证明了我们的方法的有效性。请注意，提示的采用与大多数现有的Med-VLP方法是正交的，并且可能是这些方法的有益和补充扩展。**医学图像，视觉和文本提示来统一输入格式**
23. Prompt Tuning Inversion for Text-driven Image Editing Using Diffusion Models https://arxiv.org/pdf/2305.04441.pdf
    最近，大规模语言图像模型(例如，文本引导扩散模型)大大提高了图像生成能力，可以在各个领域生成逼真的图像。基于这一成功，目前的图像编辑方法使用文本来实现直观和通用的图像修改。**要使用扩散模型编辑真实图像，必须首先将图像反转为噪声潜函数，从中对编辑后的图像进行采样，并带有目标文本提示符**。然而，大多数方法缺乏以下一项:用户友好性(例如，需要额外的掩模或输入图像的精确描述)，对更大域的泛化，或对输入图像的高保真度。在本文中，我们设计了一种精确和快速的反转技术，即提示调谐反转，用于文本驱动的图像编辑。具体来说，我们提出的编辑方法包括重构阶段和编辑阶段。在第一阶段，我们通过提示调谐反转将输入图像的信息编码为可学习的条件嵌入。在第二阶段，我们对编辑后的图像进行无分类器引导采样，其中通过在目标嵌入与第一阶段获得的优化嵌入之间进行线性插值来计算条件嵌入。这种技术确保了我们的方法的输入图像的可编辑性和高保真度之间的优越权衡。例如，我们可以在目标文本提示符的指导下改变特定对象的颜色，同时保留其原始形状和背景。在ImageNet上进行的大量实验表明，与最先进的基线相比，我们的方法具有优越的编辑性能。**文本驱动图像编辑，扩散模型**
24. CVSformer: Cross-View Synthesis Transformer for Semantic Scene Completion https://arxiv.org/pdf/2307.07938.pdf
    语义场景完成(SSC)需要准确理解3D场景中物体之间的几何和语义关系，以便对被遮挡的物体进行推理。流行的SSC方法将三维物体体素化，允许深度3D卷积网络(3D CNN)从复杂场景中学习物体关系。然而，目前的网络缺乏可控制的内核来跨多个视图对对象关系进行建模，其中适当的视图提供了提示被遮挡对象存在的相关信息。在本文中，我们提出了跨视图综合变压器(CVSformer)，它由多视图特征综合和跨视图变压器组成，用于学习跨视图对象关系。在多视图特征合成中，我们使用一组不同旋转的3D卷积核来计算每个体素的多视图特征。在交叉视图转换器中，我们采用交叉视图融合来全面学习交叉视图之间的关系，这些关系为增强单个视图的特征形成有用的信息。我们使用增强的特征来预测所有体素的几何占位和语义标签。我们在公共数据集上评估CVSformer，其中CVSformer产生最先进的结果。**场景生成，跨视图**
25. Bayesian Prompt Learning for Image-Language Model Generalization https://arxiv.org/pdf/2210.02390.pdf
    基础图像语言模型由于其通过提示学习有效地适应下游任务而引起了相当大的兴趣。提示学习将语言模型输入的一部分作为可训练的，同时冻结其余部分，并优化经验风险最小化目标。然而，已知经验风险最小化受到分布变化的影响，这损害了训练期间未见提示的泛化性。通过利用**贝叶斯方法的正则化能力，我们从贝叶斯的角度构建提示学习**，并将其表述为变分推理问题。我们的方法对提示空间进行了正则化，减少了对可见提示的过拟合，提高了对未见提示的提示泛化。我们的框架是通过以概率方式建模输入提示空间来实现的，作为先验分布，这使得我们的建议与图像上无条件或有条件的提示学习方法兼容。我们在15个基准上的经验证明，贝叶斯提示学习提供了提示空间的适当覆盖，防止学习虚假特征，并利用可转移的不变特征。这样可以更好地泛化未见过的提示，甚至可以跨不同的数据集和领域。**贝叶斯，变分推理**
26. Improving Adversarial Robustness of Masked Autoencoders via Test-time Frequency-domain Prompting https://arxiv.org/pdf/2308.10315.pdf
    在本文中，我们研究了配备BERT预训练的视觉变压器(例如，BEiT, MAE)的对抗鲁棒性。一个令人惊讶的观察结果是，MAE的对抗鲁棒性明显低于其他BERT预训练方法。这一观察结果促使我们重新思考这些BERT预训练方法之间的基本差异，以及这些差异如何影响对对抗性扰动的鲁棒性。我们的实证分析表明，BERT预训练的对抗鲁棒性与重建目标高度相关，即预测被屏蔽图像补丁的原始像素比预测语义上下文会降低模型的对抗鲁棒性，因为它引导模型更多地关注图像的中/高频成分。基于我们的分析，我们提供了一种简单而有效的方法来提高MAE的对抗鲁棒性。其基本思想是利用数据集提取的领域知识占据图像的中/高频，从而缩小对抗性扰动的优化空间。具体来说，我们对预训练数据的分布进行分组，并在频域上优化一组特定于聚类的视觉提示。在测试期间，通过基于原型的提示选择，将这些提示与输入图像结合在一起。广泛的评估表明，我们的方法明显提高了MAE的对抗鲁棒性，同时保持了其在ImageNet-1k分类上的干净性能。**掩码方法，对抗鲁棒性**
27. FS-DETR: Few-Shot DEtection TRansformer with Prompting and without Re-Training https://openaccess.thecvf.com/content/ICCV2023/papers/Bulat_FS-DETR_Few-Shot_DEtection_TRansformer_with_Prompting_and_without_Re-Training_ICCV_2023_paper.pdf
    这篇论文是关于FSOD (few - shot Object Detection)的，其中给出了一些描述新类(在训练期间未见过)的模板(示例)，目标是检测其在一组图像中的所有出现。从实际的角度来看，FSOD系统必须满足以下要求:(a)它必须按原样使用，不需要在测试时进行任何微调;(b)它必须能够同时处理任意数量的新对象，同时支持来自每个类的任意数量的示例;(c)它必须达到与封闭系统相当的精度。为了满足(a)-(c)，在这项工作中，我们做出了以下贡献:我们首次引入了一个简单但功能强大的基于视觉提示的少镜头检测变压器(FS-DETR)，它可以同时满足(a)和(b)的需求。我们的系统建立在DETR框架之上，基于两个关键思想对其进行扩展:(1)在测试期间将提供的新类的可视化模板作为视觉提示提供，(2)用伪类嵌入(类似于软提示)“戳”这些提示，然后在解码器的输出中进行预测。重要的是，我们证明了我们的系统不仅比现有方法更灵活，而且还朝着满足期望(c)迈出了一步。具体来说，它比所有不需要微调的方法都要准确得多，甚至在最完善的基准(PASCAL VOC和MSCOCO)上匹配并优于当前最先进的基于微调的方法。**小样本目标检测，DETR**
28. Introducing Language Guidance in Prompt-based Continual Learning https://arxiv.org/pdf/2308.15827.pdf 
    持续学习的目的是在不访问以前任务的数据的情况下，在一系列任务上学习单个模型。该领域最大的挑战仍然是灾难性的遗忘:在以前的任务中表现不佳。一些现有的方法依赖于昂贵的重放缓冲区来存储以前任务的数据块。这虽然很有希望，但当任务数量变大或由于隐私原因无法存储数据时，成本会变得很高。作为一种替代方法，基于提示的方法将任务信息存储在一个可学习的提示池中。此提示池指示冻结的图像编码器如何解决每个任务。虽然在这种情况下，模型在每个任务中都面临一组不相交的类，但我们认为这些类可以被编码到预训练的语言编码器的相同嵌入空间中。在这项工作中，我们提出了基于提示的持续学习语言指南(LGCL)作为基于提示的方法的插件。LGCL是模型不可知的，它在提示池的任务级和视觉编码器的输出特征的类级上引入了语言指导。我们通过大量的实验表明，LGCL不断提高基于提示的持续学习方法的性能，从而开创了新的SOTA。LGCL无需任何额外的可学习参数即可实现这些性能改进。**持续学习，预制提示池，任务级提示和类级提示**
29. Visually-Prompted Language Model for Fine-Grained Scene Graph Generation in an Open World https://arxiv.org/pdf/2303.13233.pdf
    场景图生成(Scene Graph Generation, SGG)旨在提取主语、谓语、宾语。用于视觉理解的图像关系。尽管最近的工作在SGG方面取得了稳定的进展，但它们仍然存在长尾分布问题，即尾部谓词与频繁谓词相比，由于注释数据较少，因此训练成本更高，并且难以区分。现有的重新平衡策略试图通过先前的规则来处理它，但仍然局限于预定义的条件，这对于各种模型和数据集来说是不可扩展的。在本文中，我们提出了一个跨模态谓词增强(CaCao)框架，其中学习了视觉提示语言模型，以低资源的方式生成各种细粒度谓词。提出的CaCao可以以即插即用的方式应用，并自动加强现有的SGG以解决长尾问题。在此基础上，我们进一步引入了一种新的用于开放世界谓词场景图生成(Epic)的纠缠跨模态提示方法，其中模型可以以零采样的方式泛化到未见谓词。在三个基准数据集上的综合实验表明，CaCao以模型不可知的方式持续提高多个场景图生成模型的性能。此外，我们的Epic在开放世界谓词预测方面取得了具有竞争力的性能。本文的数据和代码是公开的。**场景图生成，跨模态谓词增强**
30. Open Set Video HOI detection from Action-Centric Chain-of-Look Prompting https://openaccess.thecvf.com/content/ICCV2023/papers/Xi_Open_Set_Video_HOI_detection_from_Action-Centric_Chain-of-Look_Prompting_ICCV_2023_paper.pdf
    **人-物交互(HOI)检测**对于理解和模拟现实世界的事件至关重要。现有的HOI检测工作主要集中在静态图像和封闭设置上，其中所有的HOI类都在训练集中提供。相比之下，在开放场景中检测视频中的hoi更具挑战性。首先，在开放环境下，期望HOI检测器具有很强的泛化能力，以识别未包含在训练数据中的未见HOI。其次，从视频中准确捕获时间上下文信息是困难的，但对于检测与时间相关的动作(如打开、关闭、拉、推)至关重要。为此，我们提出了一种以动作为中心的面向开放集视频HOI检测的Chain-of-Look提示模型ACoLP。ACoLP将动作作为视频中语义的载体，跨帧捕捉重要的语义信息。受自然语言处理中的思维链提示的启发，为了使模型在未见类上具有泛化性，我们引入了视觉链提示方案，该方案将大规模视觉语言模型中的提示生成分解为一系列中间视觉推理步骤。因此，我们的模型捕获了视频中HOI事件背后的复杂视觉推理过程，为检测看不见的类提供了必要的指导。在两个视频HOI数据集(VidHOI和CAD120)上进行的大量实验表明，在传统的封闭环境中，ACoLP与最先进的方法相比具有竞争力，在开放环境中，ACoLP的性能大大优于现有方法。**人物交互检测，Chain-of-Look**
31. Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels? https://arxiv.org/pdf/2307.11978.pdf'
    像CLIP这样的视觉语言模型从大规模训练数据中学习通用的文本-图像嵌入。通过几次提示调优，可以使视觉语言模型适应新的分类任务。我们发现这种快速调谐过程对标记噪声具有很高的鲁棒性。这促使我们去研究促成提示调优范式的鲁棒性的关键原因。我们进行了大量的实验来探索这一特性，发现关键因素是:1)固定的类名令牌为模型的优化提供了强正则化，减少了由噪声样本引起的梯度;2)从多样化和通用的web数据中学习到的强大的预训练图像文本嵌入为图像分类提供了强大的先验知识。此外，我们证明了CLIP的噪声零样本预测可以用来调整它自己的提示，显著提高了无监督设置下的预测精度。**Clip对噪声的鲁棒性**
32. Read-only Prompt Optimization for Vision-Language Few-shot Learning https://arxiv.org/pdf/2308.14960.pdf
    近年来，提示调优已被证明在使预训练的视觉语言模型适应下游任务方面是有效的。这些方法旨在通过引入可学习的提示来适应预训练的模型，同时保持预训练的权重不变。然而，**可学习提示会影响自注意模块的内部表示**，这可能会对性能差异和泛化产生负面影响，**特别是在数据不足的设置中**。为了解决这些问题，我们提出了一种新的方法，只读提示优化(RPO)。RPO利用隐藏注意力来防止预训练模型中的内部表示转移。此外，为了便于优化RPO，根据预训练模型的特殊令牌初始化只读提示。我们的大量实验表明，RPO在基到新泛化和域泛化方面优于CLIP和coocoop，同时表现出更好的鲁棒性。此外，该方法在数据极度缺乏的情况下也能实现更好的泛化，同时提高了参数效率和计算开销。**只读提示**
33. Unsupervised Prompt Tuning for Text-Driven Object Detection https://openaccess.thecvf.com/content/ICCV2023/papers/He_Unsupervised_Prompt_Tuning_for_Text-Driven_Object_Detection_ICCV_2023_paper.pdf
    基于语言图像的预训练模型对各种下游目标检测任务显示出很强的零样本泛化能力。尽管这些模型的性能很好，但它们在很大程度上依赖于费力的提示工程。现有的方法通常通过使用下游训练数据以少量或完全监督的方式调整文本提示来解决这个问题。然而，一个很少研究的问题是在不使用任何注释的情况下优化文本提示。本文对这一问题进行了深入研究，提出了一种文本驱动目标检测的无监督提示调优框架，该框架由两种新颖的均值教学机制组成。在传统的均值教学中，随着训练的进行，伪盒的质量有望得到更好的优化，但仍然存在过拟合噪声伪盒的风险。为了解决这一问题，1)提出了嵌套均值教学，采用嵌套注释以双层优化的方式监督师生相互学习;2)我们提出了双互补教学(Dual Complementary Teaching)，即通过基于数据增强的互补标注，聘请一名线下的预训练教师和一名线上的均值教师，确保学习过程中不积累确认偏差。通过整合这两种机制，所提出的无监督提示调优框架在广泛的目标检测数据集上实现了显著的性能提升。**文本驱动目标检测，无监督学习**
34. Distribution-Aware Prompt Tuning for Vision-Language Models https://arxiv.org/pdf/2309.03406.pdf
    通过利用从大数据中学习到的知识，预训练的视觉语言模型(VLMs)在各种下游任务中表现出令人印象深刻的性能。通常，通过提示调优，可以进一步提高vlm在目标任务上的性能，这可以为输入图像或文本添加上下文。通过利用目标任务的数据，文献中研究了各种提示调优方法。提示调整的关键是通过固定模型参数的可学习向量在两个模态之间进行特征空间对齐。我们观察到，当每个模态的嵌入在潜在空间中“排列良好”时，对齐变得更加有效。受这一观察结果的启发，我们提出了用于视觉语言模型的分布感知提示调优(DAPT)，该方法简单而有效。具体来说，提示是通过最大化内部分散，类之间的距离，以及最小化由同一类嵌入之间的距离测量的内部分散来学习的。我们在11个基准数据集上的大量实验表明，我们的方法显着提高了泛化性。**分布感知提示**
35. March in Chat: Interactive Prompting for Remote Embodied Referring Expression https://arxiv.org/pdf/2308.10141.pdf
    近年来提出了许多视觉语言导航(VLN)任务，从基于空间到基于对象，从室内到室外。REVERIE(远程具体化引用表达式)很有趣，因为它只向代理提供高级指令，这在实践中更接近于人类的命令。然而，这比其他VLN任务更具挑战性，因为它要求智能体仅根据短指令推断导航计划。大型语言模型(llm)通过提供适当的提示，在机器人动作规划中显示出巨大的潜力。尽管如此，这个策略还没有在REVERIE设置下进行探索。有几个新的挑战。例如，LLM应该具有环境意识，以便根据当前的视觉观察调整导航计划。此外，LLM计划的动作应该适应更大更复杂的REVERIE环境。基于新提出的空间和物体感知场景感知器(ROASP)，提出了一种可以与LLM动态对话并进行动态规划的“聊天进行”(MiC)模型。在REVERIE基准测试中，我们的MiC模型在SPL和RGSPL指标方面大大优于以前的最先进的模型。**LLM，动态对话，动态规划**
36. Generative Action Description Prompts for Skeleton-based Action Recognition https://arxiv.org/pdf/2208.05318.pdf
    基于骨骼的动作识别最近受到了相当大的关注。目前基于骨架的动作识别方法通常被表述为单一的分类任务，并且没有充分利用动作之间的语义关系。例如，“做胜利手势”和“竖起大拇指”是手势的两种动作，它们的主要区别在于手的动作。这些信息与动作类的分类编码无关，但可以从动作描述中揭示出来。因此，在训练中使用动作描述可能有利于表征学习。在这项工作中，我们提出了一种基于骨架的动作识别的生成动作描述提示(GAP)方法。具体而言，我们采用预训练的大规模语言模型作为知识引擎，自动生成动作肢体运动的文本描述，并提出了一种多模态训练方案，利用文本编码器生成不同身体部位的特征向量，并监督骨架编码器进行动作表示学习。实验表明，我们提出的GAP方法在不增加推理计算成本的情况下，在各种基线模型上取得了显著的改进。GAP在流行的基于骨架的动作识别基准上实现了最新的技术水平，包括NTU RGB+D, NTU RGB+D 120和NW-UCLA。**动作识别，动作描述**
37. E^2VPT: An Effective and Efficient Approach for Visual Prompt Tuning https://openaccess.thecvf.com/content/ICCV2023/papers/Han_E2VPT_An_Effective_and_Efficient_Approach_for_Visual_Prompt_Tuning_ICCV_2023_paper.pdf
    随着基于变压器的模型的规模不断增长，为新任务微调这些大规模预训练的视觉模型已经变得越来越参数密集。参数高效学习是为了减少微调过程中可调参数的数量。尽管这些方法显示出很好的结果，但与完全微调相比，仍然存在显着的性能差距。为了解决这一挑战，我们提出了一种有效和高效的视觉提示调谐(E2VPT)方法，用于大规模基于变压器的模型自适应。具体来说，我们在自注意层和输入层分别引入了一组可学习的键值提示和视觉提示，以提高模型微调的有效性。此外，我们设计了一个提示剪枝程序，在保持模型性能的同时系统地修剪低重要性提示，大大提高了模型的效率。实证结果表明，我们的方法在两个基准上优于几个最先进的基线，参数使用量相当低(例如，VTAB-Ik上模型参数的0.32%)。**VPT**
38. PODA: Prompt-driven Zero-shot Domain Adaptation https://arxiv.org/pdf/2212.03241.pdf
    领域自适应在计算机视觉中已经得到了广泛的研究，但仍然需要在流程运行时访问目标图像，这在一些不常见的情况下可能会很棘手。在本文中，我们提出了“提示驱动的零样本域自适应”的任务，其中我们只使用目标域的自然语言的一般描述(即提示)来适应在源域上训练的模型。首先，我们利用预训练的对比视觉语言模型(CLIP)来优化源特征的仿射转换，在保留其内容和语义的同时将其引导到目标文本嵌入。为了实现这一点，我们提出了提示驱动的实例规范化(PIN)。其次，我们证明了这些提示驱动的增强可以用于语义分割的零射击域自适应。实验表明，对于手头的下游任务，我们的方法在多个数据集上显著优于基于clip的风格转移基线，甚至超过了一次性无监督域自适应。在目标检测和图像分类方面也观察到类似的提升。**领域自适应**
39. When Prompt-based Incremental Learning Does Not Meet Strong Pretraining https://arxiv.org/pdf/2308.10445.pdf
    增量学习旨在克服从顺序任务中学习深度网络时的灾难性遗忘。基于提示的方法具有令人印象深刻的学习效率和性能，通过学习特定于任务的提示，为顺序任务采用固定的主干。然而，现有的基于提示的方法严重依赖于强预训练(通常在ImageNet-21k上训练)，我们发现如果预训练任务和未知的未来任务之间的潜在差距很大，它们的模型可能会被困住。在这项工作中，我们开发了一个可学习的自适应提示生成器(APG)。关键是将提示检索和提示学习过程统一为一个可学习的提示生成器。因此，可以优化整个提示过程，有效减少任务间隙的负面影响。为了使我们的APG避免学习无效知识，我们维护了一个知识库，利用每个类的特征分布对APG进行正则化。大量的实验表明，我们的方法在没有(强)预训练的无样本增量学习中明显优于先进的方法。此外，在强再训练下，我们的方法也具有与现有基于提示的模型相当的性能，表明我们的方法仍然可以从预训练中获益。**生成式提示，预制知识库**
40. Prompt-aligned Gradient for Prompt Tuning https://arxiv.org/pdf/2205.14865.pdf
    由于像CLIP这样的大型预训练视觉语言模型(VLM)，我们可以通过“提示”来制作零射击分类器，例如，使用VLM提供的图像与提示句子“a photo of a [CLASS]”之间的相似性度量，可以获得图像为“[CLASS]”的置信分数。因此，如果我们对基于提示的相似性度量进行微调，提示显示出vlm快速适应下游任务的巨大潜力。然而，我们发现了一个常见的错误，即不适当的微调不仅会破坏提示符对任务相关类的固有预测，还会破坏VLM词汇表中其他类的预测。现有的方法仍然通过使用传统的反过拟合技术来解决这个问题，例如早期停止和数据增强，这些技术缺乏针对提示的原则性解决方案。我们提出了提示对齐梯度，称为ProGrad，以防止提示调优忘记从vlm中学习的一般知识。特别是，ProGrad只更新梯度与“大方向”对齐(或不冲突)的提示符，“大方向”表示为预定义提示符预测的KL损失的梯度。大量的实验表明，ProGrad比当前最先进的提示调谐方法具有更强的小片段泛化能力。**提示对其梯度**
41. SegPrompt: Boosting Open-World Segmentation via Category-Level Prompt Learning https://arxiv.org/pdf/2308.06531.pdf
    目前的闭集实例分割模型在训练和评估过程中依赖于每个掩码的预定义类标签，这在很大程度上限制了它们检测新对象的能力。开放世界实例分割(OWIS)模型通过以类不可知的方式检测未知对象来解决这一挑战。然而，以前的OWIS方法在训练期间完全删除类别信息，以保持模型泛化到未知对象的能力。在这项工作中，我们提出了一种新的训练机制，称为SegPrompt，它使用类别信息来提高模型对已知和未知类别的类别无关分割能力。此外，以前的OWIS训练设置将未知类暴露给训练集，带来信息泄漏，这在现实世界中是不合理的。因此，我们通过将数据集类划分为已知见过的未见过的部分，提供了一个更接近现实世界场景的新的开放世界基准。我们第一次关注模型发现从未出现在训练集图像中的对象的能力。实验表明，在我们的新基准上，SegPrompt可以在不影响推理效率的情况下将AR的整体检测性能和未见检测性能分别提高5.6%和6.1%。我们进一步证明了我们的方法在现有的跨数据集传输和强监督设置上的有效性，导致5.5%和12.3%的相对改进。**开放世界实例分割**
42. Online Class Incremental Learning on Stochastic Blurry Task Boundary via Mask and Visual Prompt Tuning https://arxiv.org/pdf/2308.09303.pdf
    持续学习旨在从连续的数据流中学习模型，但它主要假设固定数量的数据和任务，并有明确的任务边界。然而，在实际场景中，输入数据和任务的数量以统计方式而不是静态方式不断变化。尽管最近引入的具有模糊任务边界的增量学习场景在一定程度上解决了上述问题，但由于不连贯和模糊样本的固定比例，它们仍然不能完全反映现实世界情况的统计特性。在本文中，我们提出了一个新的随机增量模糊任务边界场景，称为Si-Blurry，它反映了现实世界的随机特性。我们发现在si - blur情景中存在两大挑战:(1)任务间和任务内遗忘;(2)类别失衡问题。为了减轻这些问题，我们引入了蒙版和视觉提示调优(MVP)。在MVP中，为了解决任务间和任务内遗忘问题，我们提出了一种新颖的实例逻辑掩蔽和对比视觉提示调谐损失。它们都帮助我们的模型识别当前批次中要学习的类。它的结果是巩固以前的知识。此外，为了缓解类不平衡问题，我们引入了一种新的基于梯度相似度的焦点损失和自适应特征缩放来缓解对主要类的过拟合和对次要类的欠拟合。大量的实验表明，我们提出的MVP在具有挑战性的si - blur场景中明显优于现有的最先进的方法。**持续学习，基于梯度相似度的焦点损失**
43. Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models https://arxiv.org/pdf/2308.11186.pdf
    预先训练的视觉语言模型，如CLIP，与手动设计的提示一起工作，已经证明了迁移学习的巨大能力。最近，可学习提示实现了最先进的性能，但是容易过度拟合到可见类，无法推广到未见类。本文提出了一种用于视觉语言模型的知识感知提示调优(KAPT)框架。我们的方法从人类智能中获得灵感，在人类智能中，外部知识通常被纳入识别新类别的对象。具体来说，我们为文本编码器设计了两种互补类型的知识感知提示，以利用与类别相关的外部知识的独特特征。离散提示从对象类别的描述中提取关键信息，学习的连续提示捕获整体上下文。我们进一步为视觉编码器设计了一个自适应头来聚合显著的注意视觉线索，从而建立了判别和任务感知的视觉表征。我们在11个广泛使用的基准数据集上进行了大量的实验，结果验证了该方法在少量图像分类中的有效性，特别是在推广到未见过的类别方面。与最先进的coop方法相比，KAPT表现出良好的性能，在新类上获得了3.22%的绝对增益，在谐波平均上获得了2.57%的绝对增益。**知识感知提示，双提示**
44. CoTDet: Affordance Knowledge Prompting for Task Driven Object Detection https://arxiv.org/pdf/2309.01093.pdf
    任务驱动目标检测旨在检测图像中适合提供任务的对象实例。它的挑战在于，用于任务的对象类别过于多样化，无法局限于传统对象检测的一组封闭的对象词汇表。简单地将常见对象的类别和视觉特征映射到任务中并不能解决这一挑战。在本文中，我们建议探索基本的可视性而不是对象类别，即使不同对象能够完成相同任务的共同属性。此外，我们还提出了一种新的多级思维链提示(MLCoT)从大型语言模型中提取提示知识，该模型包含从任务到对象示例再到基本视觉属性的多级推理步骤。此外，为了充分利用知识对目标识别和定位有利，我们提出了一种知识条件检测框架，即CoTDet。它根据知识约束检测器以生成对象查询和回归框。实验结果表明，我们的CoTDet持续且显著地优于最先进的方法(+15.6盒AP和+14.8掩模AP)，并且可以生成为什么检测对象以承担任务的基本原理。**任务驱动目标检测，思维链提示**
